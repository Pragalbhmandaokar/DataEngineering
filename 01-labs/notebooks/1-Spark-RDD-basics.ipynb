{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6ae9dce-7b87-405f-a70e-836aa384a70c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Working with RDDs (ch 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de6f9eda-ccbf-4457-afa2-4335464af50c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "You control a Spark process by means of a Spark Context. In Python, the `Spark context` is a built-in variable, already bound to the Context. When using Java (or Scala), you need to do the binding yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ede1e39-9b52-4a72-8b6d-102a08d508c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72c68eb8-11a8-403f-ae79-39be8e4dbaec",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "you will use `sc` to ask Spark to load the content of a file into memory, for instance:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "578d2a88-a437-40da-ba46-8f491ad1d404",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## RDD: Resilient Distributed Dataset ##\n",
    "\n",
    "The basic data abstraction for all Spark programs is the RDD. Think of an RDD as an immutable, distributed collection of objects (data elements) of a certain type.\n",
    "\n",
    "Users create RDDs in two ways: by loading an external dataset, as we have just seen, or programmatically by specifying that a collection of objects, for instance a list or set, is to be processed a set of N *workers*:\n",
    "\n",
    "note that Spark 2.x now support a higher level abstraction for relational data, called **DataFrames**. if you are familiar with Python Pandas or the R language, you will recognise Spark DataFrames as a very similar abstraction for tabular data. We will look at DataFrames in details later. For now, you may  refer to this tutorial: [Spark SQL, DataFrames and Datasets Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3635bcb-42ff-4e2c-8b6e-6447f9c91a3d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# specifies the number of workers:\n",
    "myData = sc.parallelize([1,2,3,5],3)\n",
    "myData.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "140ea2d5-7b5e-470d-be40-b213270e874d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# distributes the content of the RDD to all available workers:\n",
    "\n",
    "sc.parallelize([\"pandas\", \"i like pandas\"]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4ed4957-2cf5-400c-b243-223f65c83a9f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Example (3.1): create a new RDD by loading a text file using the SparkContext sc\n",
    "lines = sc.textFile('/FileStore/tables/Dante_Inferno.txt') \n",
    "lines.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fffd3201-53f7-4f5a-a27d-d02f5a93b372",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Transformations and Actions ##\n",
    "\n",
    "Spark operates on RDDs in two ways:\n",
    "\n",
    "- through **transformations**. A transformation takes an input RDD and produces a new RDD. Example: filtering an RDD \n",
    "- through **actions**. An Action takes an RDD and produces data of some other type, which typically encodes the result of some data analysis. Example: count the number of lines in a file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb6944b9-99eb-487a-9277-d363a1ffe3b5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "example of transformation: \n",
    "\n",
    "the myRDD.filter() function filters myRDD according to a condition. The condition is specified using a function as one of the arguments to filter(). Here is the general Python syntax for passing a `lambda function` to another function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a16718c-835d-48ff-913b-6dc5519283c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "infernoLines = lines.filter(lambda x: \"inferno\" in x)\n",
    "infernoLines.collect()\n",
    "infernoLines.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca4ab6d9-5a52-4d5c-ac47-a2b165d67ec3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Action: a result computed from an RDD, which can be either returned to the driver program or saved to an external storage system (e.g., HDFS).  examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df2c9e69-bd0b-4a79-906b-f095a98c176a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# operates on the lines RDD\n",
    "lines.count()\n",
    "lines.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05c4173d-db62-407a-ad82-8c7395f0975e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# shows top k elements in the RDD \n",
    "lines.take(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f67d73b-2d60-4145-9d11-255e96aa7386",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Actions are typically performed at the end of a pipeline consisting of one or more transformations.\n",
    "\n",
    "Here is a new transformation that makes use of the `map()` higher order function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d5e6e03-36f1-41ae-88b1-1b475dad9721",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "uppercaseLines = lines.map(lambda s: s.upper())\n",
    "uppercaseLines.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8aaba546-23ef-42cb-b78a-edfe1852a470",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "like filter,  `map()` is a transformation. It takes an input function (a named or a lambda function). It applies the function to each element of the input RDD to produce an output RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6302a94-b6ca-4080-8cd0-bf3742a0c83d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "uppercaseLines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18d7a739-f30e-4061-990e-2090a2cbcbca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "uppercaseLines.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84a11020-4e50-4c64-8102-25d03e7c3bcf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "infernoLines.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2dffc7d-5cdd-4824-8db8-b49c6603afd7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "note that the result of the take(n) action function is no longer an RDD! in this instance, it is a list of strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eda06e72-9c57-45b6-9bcf-eaac2d226000",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "note that you may concatenate the two transformations and the action into a single command using 'dot notation':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99fdc5bd-1dd3-4d28-8b54-8a94475a5e80",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "lines.filter(lambda x: \"inferno\" in x).map(lambda s: s.upper()).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1a2b972-2ab3-41b0-b847-e8a4eb5579b8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "or you can use the functions you have defined to perform these operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5459f408-fbc6-45f0-be2d-02e9d1e8d8c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def upperCase(doc):\n",
    "    return doc.map(lambda s: s.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ccc7eac-304a-45ee-af90-e944ca9fc563",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def filterDocForTerm(doc, term):\n",
    "    return doc.filter(lambda x: term in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c3d485b-9330-4a08-8ae3-9a4c4bb9c2e0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "upperCase(filterDocForTerm(lines, \"inferno\")).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a755beb-3bb1-4711-8b95-ec317a1ae8e3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "we can also explicitly define functions that we can then use in `map()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b1b8fb6-bde4-4791-b143-63dd6d3ab895",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def removex92(s):\n",
    "    return s.replace('\\x92','\\'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c48c0b66-e81b-4a14-8c12-aedd7b9e644b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "lines.filter(lambda x: \"inferno\" in x).map(removex92).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fa7ec70-5146-4ea6-a4d6-6a0980337b16",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "or equivalently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13783c05-3b86-438b-bcc0-a76711bf5fee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "lines.filter(lambda x: \"inferno\" in x).map(lambda s: s.replace('\\x92','')).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61de86b6-de09-4b57-9084-c2e03e4cef75",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# other obvious action: counting the number of elements\n",
    "infernoLines.count(), uppercaseLines.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5624c995-9936-41a3-a6f0-6c8afdc454a4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Lazy evaluation:##\n",
    "\n",
    "Spark computes a pipeline in a **lazy** fashion—that is, the resulting RDD remains virtual and no computation  actually occurs until the RDD is used as input to an action.\n",
    "\n",
    "Thus, actions have the potentual to trigger an entire complex pipeline to be executed.\n",
    "\n",
    "If Spark were to load and store all the lines in the file as soon as we wrote lines = sc.textFile(...), it would waste a lot of storage space, given that we then immediately filter out many lines. Instead, once Spark sees the whole chain of transformations, it can compute just the data needed for its result. In fact, for the first() action, Spark scans the file only until it finds the first matching line; it doesn’t even read the whole file.\n",
    "\n",
    "Note that a virtual computation may be described by a graph, for instance:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e412690-4c67-4f52-bcbe-e21462ae9d8f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "try changing the file path below to a non-existent name like `/FileStore/tables/Dante_Inferno-XXX.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d2f5409-9a10-4dd3-92e2-c0d73bf98f3e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "lines = sc.textFile('/FileStore/tables/Dante_Inferno.txt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e03adb31-b013-4901-913c-d84a66cef6da",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cleanLines = lines.map(lambda s: s.replace('\\x92',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "015a7353-3380-4ded-bfea-ae5273dc7ce5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "upperInferno = cleanLines.filter(lambda x: \"inferno\" in x).map(lambda s: s.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9133168-cbe9-4a85-8d97-4cbede1c2913",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "lowerAmore = cleanLines.filter(lambda x: \"amore\" in x).map(lambda s: s.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c37bfe7b-4d13-4669-9bcc-416c89c0a266",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "at this point, no computation has actually occurred. In fact the file hasn't even been opened (try and change the path to the file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88385678-f6be-41b7-82a4-dc67dcc2177f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "however when an action is performed on *at least one* of the output RDDs:  `upperInferno`, `lowerAmore`, Spark schedules the entire execution graph in a distributed fashion using the available workers, in order to produce a result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5793e093-6fb8-47dc-b052-35cab3dfb638",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "upperInferno.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35c8e601-49fa-4dd3-8314-b7fba0d03f20",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "lowerAmore.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fc09949-d285-4d88-9b6b-7ccf0c152758",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "have you tried using the wrong filename?  at which point do you get a runtime error?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "424474b2-31bb-462e-999a-b093f6b82792",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Persistence: ##\n",
    "    \n",
    "A RDD that is used in multiple actions, is recomputed as part of the pipeline that leads to that action. There is therefore a potential for re-computing the same RDD multiple times. \n",
    "\n",
    "However we may tell Spark that it should reuse a partial result when we know it is safe to do so. This is done using `RDD.persist()`. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b10d5fa1-a56b-4c7c-ad52-733a47a31b1e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# without persistence, this action causes the entire pipeline to be executed again:\n",
    "upperInferno.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77d2d7fd-988f-4a57-a66b-e038318b57d1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# this ensures that the file is only loaded once and that the cleanLines RDD is kept in memory:\n",
    "cleanLines.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e00ae94-514b-46fb-bcda-b37b80af7cb4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "obviously there is a trade-off between the cost of reloading / recomputing and the process space needed for storing a persisted RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7511765c-95e8-4c3c-ae75-6a4164e89ffb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "upperInferno.persist()\n",
    "lowerAmore.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7a8e035-94d7-4e3d-bc48-42fff040e117",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# these repeated action invocationsare now inexpensive:\n",
    "upperInferno.count(), lowerAmore.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb5a2598-2242-4b84-a956-5f64331897f3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# practice: loading and operating on a movie set #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f343b02-e900-4a36-b48f-334663099af2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We now practice these notions on a different dataset, and extend our overview of the set of transformations and actions avaiable through Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04d51005-c821-4cd6-9cd5-eb583e194ccf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "    inputRDD = sc.textFile('/FileStore/tables/sample_movielens_movies.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "991603ac-b30b-4222-85aa-0a08ca9e65f2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "inputRDD.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6888f61-1927-4cde-8978-7ce36c7b51ab",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Examples of **transformations**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a36b010-fc65-451d-b04b-3287e717dd5d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "thrillerRDD = inputRDD.filter(lambda x: \"Thriller\" in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bf42b19-4f0d-47d7-880e-1ada129cd308",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "comedyRDD = inputRDD.filter(lambda x: \"Comedy\" in x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b206879-860d-4f3b-b3da-c571ed7d80e7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "you can take the **union** of two RDDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ff8d31a-148f-40d9-8237-08853f9cab8a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "whatILikeRDD = thrillerRDD.union(comedyRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8c1cceb-1b86-4030-b093-4740453c2bb1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "*Actions* - printing out the first 10 lines of each movie category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d9fcc65-8228-46c9-8ea5-d6b4d8d37bf2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "thrillerRDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d1cda00-8e19-4cbb-9785-2db4d23e277b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "comedyRDD.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9928c1d8-f26d-42e0-87fb-3401a9c0b83f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "whatILikeRDD.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce6f3adc-5bd9-431d-8f8d-0de3d69b9d39",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "note: the `collect()` function retrieves the entire RDD.\n",
    "\n",
    "*Note that this forces Spark to execute the entire pipeline* so not to be used on large datasets unless you know that processing requires the entire dataset to be consumed outside of the RDD framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a0b1e43-1520-4b0b-9c41-2152a3002b74",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "whatILikeRDD.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8f92c0f-8381-4ba4-a3b4-5b79590941fd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Passing functions to Spark** (ch 3 pg. 30). we have seen this before..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7add3c61-9994-42c4-90ba-dfbe458dbbf2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def isThriller(s): \n",
    "    return \"Thriller\" in s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56eff8be-fce1-476d-93f1-93b9e32597f1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "thrillers = whatILikeRDD.filter(isThriller)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7100b3f7-c4c2-42fa-bd18-028d06282da3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "thrillers.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c03e8f79-a0a8-46c4-b878-e9acb05e232d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1-Spark-RDD-basics",
   "widgets": {}
  },
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
