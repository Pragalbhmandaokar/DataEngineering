{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6169af9e-c020-42ef-9497-2cbb3ae0d59a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## CSC8101 - Practical 07 Feb 2023\n",
    "\n",
    "#### Exercise 2 - Movies Dataset - Take home\n",
    "\n",
    "Two input [datasets](https://www.kaggle.com/datasets/rounakbanik/the-movies-dataset):\n",
    "\n",
    "- `ratings` dataset: Movie ratings from 270,000 users for all 45,000 movies. Ratings are on a scale of 1-5 and have been obtained from the official GroupLens website.\n",
    "- `movies` dataset: The main Movies Metadata file. Contains information on 45,000 movies featured in the Full MovieLens dataset.\n",
    "\n",
    "Each of these datasets is read into a DataFrame below.\n",
    "\n",
    "##### Task 1\n",
    "\n",
    "1. How many partitions has each dataset?\n",
    "2. How big is each dataset? (Report number of rows)\n",
    "3. Repartition the `ratings` dataset by key `movieID` across `100` partitions.\n",
    "4. Verify that the `ratings` dataset now has `100` partitions.\n",
    "\n",
    "Docs:\n",
    "- [Repartition](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.repartition.html)\n",
    "\n",
    "##### Task 2\n",
    "\n",
    "Write a data pipeline function that takes as input the two datasets above and outputs the `N` most popular films (by rating), for a given `genre`, for a given `decade` (specified by its start year, e.g. `1980` for 80s and `2000` for 2000s).\n",
    "\n",
    "> Example function run: `pipeline(N = 10, genre = \"comedy\", decade = 2010)`\n",
    "\n",
    "Run your function for the following parameter inputs and report your findings. Set `N = 10` throughout:\n",
    "\n",
    "- `genre = \"Thriller\"`, `decade = 1980`\n",
    "- `genre = \"Drama\"`, `decade = 2000`\n",
    "- `genre = \"Comedy\"`, `decade = 2010`\n",
    "\n",
    "Helpful docs:\n",
    "\n",
    "- [DataFrame quickstart](https://spark.apache.org/docs/latest/api/python/getting_started/quickstart_df.html?highlight=select)\n",
    "- [withColumn](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.withColumn.html?highlight=withcolumn#pyspark.sql.DataFrame.withColumn)\n",
    "- [select](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.select.html#pyspark.sql.DataFrame.select)\n",
    "- [orderBy](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.orderBy.html?highlight=orderby#pyspark.sql.DataFrame.orderBy)\n",
    "- [join](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.join.html?highlight=join#pyspark.sql.DataFrame.join)\n",
    "- [filter](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.filter.html?highlight=filter#pyspark.sql.DataFrame.filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dec4e3ad-352d-485f-b08c-0bc1fd0eafa8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as FN\n",
    "import pyspark.sql.types as TP\n",
    "\n",
    "# Task 1\n",
    "\n",
    "# File location and type\n",
    "ratings_file_location = \"/FileStore/tables/movies/ratings.csv\"\n",
    "movies_file_location = \"/FileStore/tables/movies/movies_metadata.csv\"\n",
    "file_type = \"csv\"\n",
    "\n",
    "# CSV options\n",
    "infer_schema = \"true\"\n",
    "first_row_is_header = \"true\"\n",
    "delimiter = \",\"\n",
    "\n",
    "\n",
    "\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "ratings = spark.read.format(file_type) \\\n",
    "          .option(\"inferSchema\", infer_schema) \\\n",
    "          .option(\"header\", first_row_is_header) \\\n",
    "          .option(\"sep\", delimiter) \\\n",
    "          .load(ratings_file_location)\n",
    "\n",
    "movies_metadata = spark.read.format(file_type) \\\n",
    "                  .option(\"inferSchema\", infer_schema) \\\n",
    "                  .option(\"header\", first_row_is_header) \\\n",
    "                  .option(\"sep\", delimiter) \\\n",
    "                  .load(movies_file_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e252dbd0-639d-4d59-a575-0b3ab613b280",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(ratings.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b17f9f0d-74be-4713-9994-b0b7fb2c38d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(movies_metadata.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21eeb86c-0a5b-4c78-9077-eb30df743d21",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "movies_metadata[['genres']].take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62c44126-21b5-4e2a-8428-88002837e296",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initial pre-processing\n",
    "\n",
    "## Select relevant columns\n",
    "movies = movies_metadata[['id','original_title','genres','release_date']]\n",
    "\n",
    "## sample data\n",
    "display(movies.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af1345db-30c4-4534-875a-d50dbe5851be",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Notice that genres is a string but that's not very useful - so we need to make it a structure that can be \n",
    "# In this case that is an array of dicts with key set ('id', 'name')\n",
    "\n",
    "\n",
    "# schema for 'genres' column in movies metadata dataset\n",
    "genres_schema = TP.ArrayType(\n",
    "    TP.StructType([\n",
    "        TP.StructField(\"id\", TP.IntegerType()),\n",
    "        TP.StructField(\"name\", TP.StringType())\n",
    "    ])\n",
    ")\n",
    "\n",
    "# Now we overwrite columns 'genres' to parse the string into a data structure that we can manipulate\n",
    "movies = movies.withColumn(\"genres\", FN.from_json(movies.genres, genres_schema))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f81c019c-563b-41ad-98bd-fba1e682c5f3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(movies.printSchema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a19e2589-a1d5-416d-a581-93996f8fc297",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Task 1 - Partition `ratings` dataset\n",
    "\n",
    "1. How many partitions has each dataset?\n",
    "2. How big is each dataset? (Report number of rows)\n",
    "3. Repartition the `ratings` dataset by key `movieID` across `100` partitions.\n",
    "4. Verify that the `ratings` dataset now has `100` partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8d3821b-a955-408e-abc6-e60c31817d90",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# write your solution here\n",
    "# SubTask -> 1\n",
    "RDD = movies.rdd.getNumPartitions()\n",
    "print(RDD)\n",
    "\n",
    "# SubTask -> 2\n",
    "movies.rdd.count()\n",
    "print(\"Number of rows for each dataset movies:\",movies.rdd.count())\n",
    "\n",
    "\n",
    "# Subtask -> 3 \n",
    "ratings_repartitioned = ratings.repartition(100,\"movieId\")\n",
    "print(ratings_repartitioned.rdd.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee67d06c-0682-4dbb-b30b-458745ac00b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SubTask -> 4 \n",
    "sizeOfRePart = ratings_repartitioned.rdd.getNumPartitions()\n",
    "print(\"Number of count by repartition: \", sizeOfRePart)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da0fee41-89f0-45f1-9c00-09b3e150bfde",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Task 2 - Pipeline\n",
    "\n",
    "Write a data pipeline function that takes as input the two datasets above and outputs the `N` most popular films (by rating), for a given `genre`, for a given `decade` (specified by its start year, e.g. `1980` for 80s and `2000` for 2000s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5525c5e-5414-4004-b3f7-65d413ce3d2e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def movies_pipeline(movies_df, ratings_df, N = 10, genre = \"Comedy\", decade = 1980):\n",
    "    # write your solution here\n",
    "    # develop your solution in separate cells before implementing this function    \n",
    "    new_ratings_df = ratings_df.sort().take(N)\n",
    "    new_ratings_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69c83bec-9cd6-4b31-bc0d-e649ce284cc9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 2a. Begin by further pre-processing the movies dataframe\n",
    "\n",
    "Save the output of this sequence of operations into a new variable.\n",
    "\n",
    "- Extract the name of each genre in column `genres`\n",
    "- Convert date string to datetime structure in column `release_date`\n",
    "- Remove movies with null date values\n",
    "- Create new column `year` from `release_date`\n",
    "- Drop the `release_date` column\n",
    "\n",
    "Docs:\n",
    "\n",
    "- [Operations on columns - Pyspark functions](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html) (used within `withColumn` or `select`)\n",
    "- [Operations on DataFrames](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/dataframe.html)\n",
    "- [withColumn](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.withColumn.html?highlight=withcolumn#pyspark.sql.DataFrame.withColumn)\n",
    "- [filter](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.filter.html?highlight=filter#pyspark.sql.DataFrame.filter)\n",
    "- [drop](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.drop.html#pyspark.sql.DataFrame.drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d432f18c-46c2-4761-a246-eb903c041817",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# write your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1af93dbb-c7a5-45cd-a3ee-78701319e2d6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 2b. Filter the processed movies dataset based on parameters `genre` and `decade`\n",
    "\n",
    "Save the output of this sequence of operations into a new variable.\n",
    "\n",
    "Docs:\n",
    "\n",
    "- [Operations on columns - Pyspark functions](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html) (used within `withColumn` or `select`)\n",
    "- [filter](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.filter.html?highlight=filter#pyspark.sql.DataFrame.filter)\n",
    "\n",
    "Hints:\n",
    "\n",
    "- Each film can have multiple genres. What is the pyspark function that allows you to find whether an array contains an element?\n",
    "- How can we calculate that a given year is part of a decade? There's a simple mathematical formula.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9491e34-4a3c-4f37-8c14-9f206f76346e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# write your solution here\n",
    "genreMovies = movies.select()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b523417b-ca02-4f03-9c08-590d54e768ce",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 2c. Calculate the average rating of each film in the ratings dataset\n",
    "\n",
    "Save the output of this opepration into a new variable.\n",
    "\n",
    "Docs:\n",
    "\n",
    "- [groupBy](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.groupBy.html#pyspark.sql.DataFrame.groupBy)\n",
    "- [agg](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.agg.html#pyspark.sql.DataFrame.agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8156346d-10c7-40a0-b233-1a2bf8ed2bb5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# write your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9857070-de5c-47ab-9d83-9752ef9bb67c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 2d. Join the result of 2b with 2c, order by avg rating (desceding order) and select top N\n",
    "\n",
    "Docs:\n",
    "\n",
    "- [join](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.join.html?highlight=join#pyspark.sql.DataFrame.join)\n",
    "- [orderBy](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.orderBy.html?highlight=orderby#pyspark.sql.DataFrame.orderBy)\n",
    "- [select](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.select.html#pyspark.sql.DataFrame.select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1d088bd-b4cf-4f5f-b375-aa74502c8c65",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "N = 10\n",
    "\n",
    "result = subset.join(avg_ratings, subset.id == avg_ratings.movieID, how = \"inner\")\\\n",
    "               .orderBy(FN.col('avg_rating').desc())\\\n",
    "               .select(['original_title', 'year', 'avg_rating'])\\\n",
    "               .take(N)\n",
    "\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09e59fd2-a7f1-44db-a0c4-28875ee258ce",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 2e. Put everything together in a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d5ef31c-ef99-4f22-92af-ae45f75ce889",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# copy the function signature above and write your solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05e7914f-e9a9-46e8-b3de-73415df75874",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "    movies_pipeline(movies, ratings, genre = \"Thriller\", decade = 1980, N = 10)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "movies-dataframe-exercise",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
