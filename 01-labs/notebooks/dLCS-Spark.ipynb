{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bfd0403a-2a17-421e-827e-17cd4dfb1c75",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Init: configure executors based on our cluster size\n",
    "\n",
    "docs: \n",
    "- https://learn.microsoft.com/en-us/azure/databricks/clusters/cluster-config-best-practices\n",
    "- https://spark.apache.org/docs/latest/configuration.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a8b0eaa-23b5-41c8-9386-6d97a2cf2a30",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local\").setAppName(\"My app\")\n",
    "conf.set(\"spark.executor.instances\", \"256\")\n",
    "conf.set(\"spark.executor.cores\", \"2\")\n",
    "conf.set(\"spark.dynamicAllocation.initialExecutors\",16)\n",
    "\n",
    "\n",
    "conf.getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8a25c9b-7f29-4b64-8647-5e54fe50cea2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "source files: \n",
    "- raw patient sequences (one record --> one  patient)\n",
    "- crossjoin files (one record --> one pair of patients)\n",
    "- computed distances (dlcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d02b31bf-d6f0-492d-afe7-dbda599a05e6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raw_sampled_100    = '/FileStore/tables/patients-sequences/patients_df_sampled_100.csv'\n",
    "raw_sampled_1000   = '/FileStore/tables/patients-sequences/patients_df_sampled_1000.csv'\n",
    "raw_sampled_10000  = '/FileStore/tables/patients-sequences/patients_df_sampled_10000.csv'\n",
    "# raw_sampled_100000 = '/FileStore/tables/patients-sequences/patients_df_sampled_100000.csv'\n",
    "\n",
    "crossjoin_partitioned_100    = '/FileStore/tables/patients-sequences/crossjoin_partitioned_100.parquet'\n",
    "crossjoin_partitioned_1000   = '/FileStore/tables/patients-sequences/crossjoin_partitioned_1000.parquet'\n",
    "crossjoin_partitioned_10000  = '/FileStore/tables/patients-sequences/crossjoin_partitioned_10000.parquet'\n",
    "# crossjoin_partitioned_100000 = '/FileStore/tables/patients-sequences/crossjoin_partitioned_100000.parquet'\n",
    "\n",
    "dlcs_results_100    = '/FileStore/tables/patients-sequences/dlcs_partitioned_100.parquet'\n",
    "dlcs_results_1000   = '/FileStore/tables/patients-sequences/dlcs_partitioned_1000.parquet'\n",
    "dlcs_results_10000  = '/FileStore/tables/patients-sequences/dlcs_partitioned_10000.parquet'\n",
    "# dlcs_results_100000 = '/FileStore/tables/patients-sequences/dlcs_partitioned_100000.parquet'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59c944c5-cb58-45e2-8b60-801aca9265f4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "underpinning dLCS function. in terms of analytics, this is a \"heavyweight\" UDF that is applied to each record in any of the datasets above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "887c1164-c216-4ad9-911d-f1218eb7fe4c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import FloatType, BooleanType, IntegerType\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92ee1a46-ee2e-4cfb-ab9e-6a2fa37aefd2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## simple example to show UDF registration mechanims\n",
    "\n",
    "def testF (x,y):\n",
    "    return len(x) > len(y)\n",
    "\n",
    "testF_udf = F.udf(testF, BooleanType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c950300d-35c8-4882-a00a-a6fd84d4998c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## this is the distance function \n",
    "## approx matching of two sequences of tokens\n",
    "## input tokelist{1,2} are two lists of tokens. \n",
    "## return int distance between the two lists\n",
    "\n",
    "def dlcs(tokenList1, tokenList2):\n",
    "    # find the length of the lists\n",
    "    m = len(tokenList1)\n",
    "    n = len(tokenList2)\n",
    "\n",
    "    # declaring the array for storing the dp values\n",
    "    L = [[None]*(n + 1) for i in range(m + 1)]\n",
    " \n",
    "    \"\"\"Following steps build L[m + 1][n + 1] in bottom up fashion\n",
    "    Note: L[i][j] contains length of LCS of X[0..i-1]\n",
    "    and Y[0..j-1]\"\"\"\n",
    "    for i in range(m + 1):\n",
    "        for j in range(n + 1):\n",
    "            if i == 0 or j == 0 :\n",
    "                L[i][j] = 0\n",
    "            elif tokenList1[i-1] == tokenList2[j-1]:\n",
    "                L[i][j] = L[i-1][j-1]+1\n",
    "            else:\n",
    "                L[i][j] = max(L[i-1][j], L[i][j-1])\n",
    " \n",
    "    # L[m][n] contains the length of LCS of X[0..n-1] & Y[0..m-1]\n",
    "    return m+n- 2 * L[m][n]\n",
    "\n",
    "\n",
    "dlcs_udf = F.udf(dlcs, IntegerType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6827f2df-bb0c-45df-a043-66fdc653682c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### set the desired scale for the next experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4fa59653-711f-4bf3-91f1-c74b64e0e5ef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raw_input_dataset = raw_sampled_1000\n",
    "crossjoin_dataset = crossjoin_partitioned_1000\n",
    "results_dataset   = dlcs_results_1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a5fd61a-f12f-4ac2-a97d-b3f2a7475ec7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Spark `DataFrames` approach\n",
    "\n",
    "so we manage relational tables throughout\n",
    "\n",
    "the key is to partition the data so that each executor can operate on one partition, concurrently and independently from the others. \n",
    "note there are no \"reduce\" steps here, ideally the set of all distances can be computed in an \"embarassingly parallel\" fashion\n",
    "\n",
    "in theory, it should be possible to scale up the resources with the size of the input dataframe. As we will see below, in practice there are limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a88dfa55-b297-46bd-ad42-45a406b55043",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Data partitioning\n",
    "\n",
    "firstly, we need to prepare the raw dataset (dataframe). \n",
    "\n",
    "This involves two steps:\n",
    "- computing the crossjoin (cartesian product) of the dataset with itself, producing records of pairs\n",
    "\n",
    "- partitioning the crossjoin dataset , so that each executor can operate on one partition independently of the others.\n",
    "\n",
    "We save the partitioned version of the raw dataset to DBFS on the assumption that it will be used multiple times. \n",
    "\n",
    "Partitoning is an expensive operation but we only need to perform it once for each input dataset (100, 1000, etc).\n",
    "\n",
    "#### Steps:\n",
    "1. create DataFrame from raw csv file\n",
    "2. lazy compute cartesian product (N^2 records)\n",
    "3. repartition the product dataset in memory\n",
    "4. save crossjoin file to dbfs --> **we use the more efficient binary parquet format**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "831a323e-acdc-410f-808c-36daa8f7990f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 1. create DataFrame from raw csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f35d691-a431-4130-a059-d035109c2ad2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sequences_df = spark.read.csv(raw_input_dataset, header=True, inferSchema=True)\n",
    "sequences_df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3e29c9b-72d6-4276-a307-d0ad6b82b130",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## \"clean up\" the dataframe schema\n",
    "\n",
    "sequences_1 = sequences_df.drop('_c0')\n",
    "sequences_2 = sequences_1.withColumnRenamed('patient_id', 'patient_id_1').withColumnRenamed('read_2', 'read_2_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6b8755d-fd34-49b6-80ff-d1c49da1d20e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 2. lazy compute cartesian product (N^2 records)\n",
    "\n",
    "crossjoin on the dataframe with itself.\n",
    "\n",
    "note that this is a \"lazy\" operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b69d2f97-b0b3-45bc-b6d6-f3980453ff48",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sequence_pairs = sequences_1.crossJoin(sequences_2)\n",
    "sequence_pairs.count() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23c89b52-fc56-4c81-9dec-07a37de8f550",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 3. repartition the product dataset in memory\n",
    "\n",
    "data partitioning on the product dataset\n",
    "\n",
    "see https://sparkbyexamples.com/spark/spark-partitioning-understanding\n",
    "\n",
    "note that repartition is also lazy\n",
    "\n",
    "several strategies are available to define partitons. A common way is to specify one or more partition columns. This works well if the downstream aggregstions use those columns for \"groupby\".\n",
    "\n",
    "In our case it makes sense to use the combination of patient IDs, as we are going to group to group by patient id later, however this will generate N= num of patients partitions, which are too many. \n",
    "\n",
    "Thus we limit to 200 random partitions for this exercise.\n",
    "\n",
    "feel free to experiment with different partitioning strategies, and see what performs better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a82f6fbc-a1cd-42b0-85b0-2358a95e4af5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "partitioned_sequence_pairs = sequence_pairs.repartition(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "127c85f3-6dca-48fa-a12b-bcb4c48b6da3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 4. save crossjoin file to dbfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06d2f3f6-b3b8-4dd4-92d5-e17cbeeb2f27",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## following command is a Spark _action_: it triggers the actual repartitioning \n",
    "\n",
    "## for large datasets, this is an expensive operation, and we only execute it once when preparing the data for repeat analysis\n",
    "\n",
    "partitioned_sequence_pairs.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88ba6cb4-1d9b-4776-abac-0a943b916077",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "partitioned_sequence_pairs.write.mode(\"overwrite\").parquet(crossjoin_dataset)\n",
    "\n",
    " ## format(\"delta\").save(sequences_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "508394d3-8940-42bd-b905-e43f4a19ae78",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### entry point if partitioned crossjoin datasets are already on FS:\n",
    "\n",
    "if the crossjoin datasets have been previously saved to DBFS, we can start here by loading them rather than repeat the prep steps above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4149b83c-1d40-4340-8f8e-ddd23205205f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "partitioned_sequence_pairs = spark.read.parquet(crossjoin_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d9ad1f1-980e-43a6-b2db-a536bfc5ed29",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Computing distances using UDFs on DataFrames: \n",
    "\n",
    "1. compute distances on each partition. this adds a `dlcs` column to each record, holding the distance\n",
    "- in principle, each computation is independenr from all others\n",
    "- in Spark this is again lazily evaluated\n",
    "2. write the resulting dataframe back to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aef496b3-72cb-4a3d-8d8c-1c5d2a86670f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "partitioned_sequence_pairs_dlcs = partitioned_sequence_pairs.withColumn('dlcs', dlcs_udf('read_2','read_2_1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f973a5fe-6896-45ee-93c5-ad0c84a47b8d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### example aggregations:\n",
    "\n",
    "no computation has happened yet at this point, but we can illustrate how workers can deliver valuable analytics without the need to centralise the result, i.e., moving all data back to the driver node\n",
    "\n",
    "Aggregations are optimised by Spark to operate on the partitions, i.e., by having the driver request data from the workers, as needed. \n",
    "\n",
    "Thus as long as we never call `collect()` on the computed dataset, we can still be relatively efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ea4444e-1c1f-4dfd-a216-edd2005de50e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### show K random records from the result dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e603575b-aacf-4044-b277-822565a966b9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## top-k is relatively inexpensive:\n",
    "\n",
    "partitioned_sequence_pairs_dlcs.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ccfcef21-5669-4e4c-8004-0d6cf311e1df",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### compute average, min and max distances across the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15c616c1-091a-4f6a-b908-c817ae74b398",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "grouped = partitioned_sequence_pairs_dlcs.select('dlcs').groupBy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d93dc704-f543-432c-a2a4-787619922e48",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "grouped.avg().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7492d7f-5704-4ab1-a6ff-98604b90d2c8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "grouped.max().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3cd241a7-992f-4e40-8aa7-84690cb86f93",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### avg distance of each patient from all others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06b62d63-879d-4247-a753-b739609cf9ab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## group by first patient, take avg distance\n",
    "\n",
    "avg_distance_by_patient = partitioned_sequence_pairs_dlcs.groupby('patient_id').agg(F.avg('dlcs').alias('avg_dist')).orderBy('avg_dist', ascending= False)\n",
    "avg_distance_by_patient.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf6490cd-0fab-4fbb-b62d-eff4567afa8e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(avg_distance_by_patient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e50e4493-7051-40d8-8f81-5ad2a086fe13",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(avg_distance_by_patient.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5582cbd4-264f-4279-ab7a-413e38a88c21",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "select patients who have a small average distance from all others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94038f5c-a752-4297-b095-e1ed0dc421a3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dense_patients = avg_distance_by_patient.filter(avg_distance_by_patient.avg_dist < 65 - 35/2)\n",
    "\n",
    "dense_patients.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e65f4273-5d0e-43a8-8000-82e56e48fb1d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dense_patients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0983d8c5-aaf3-42b2-a790-4adf1b23f0a8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### illustrate a SQL interface to querying dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "364cab6f-5800-4161-88bb-cdd195084085",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "partitioned_sequence_pairs_dlcs.createOrReplaceTempView(\"patient_distances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b97c226-91d3-4ec0-80c8-8c0b23fc2ca0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT avg(dlcs) FROM patient_distances\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe5e1d13-1350-4cfb-bea1-7922e2c322c2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT avg(dlcs) AS avg_dist FROM patient_distances GROUP BY patient_id ORDER BY avg_dist DESC. LIMIT 20\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ac2b8ec-cfa8-4e7f-abbb-2edd142061bd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## write result dataset back to disk\n",
    "\n",
    "writing a partitioned dataset to disk does not require to move its contents back to the driver. \n",
    "\n",
    " -- if the file is naturally partitioned, data may not need to be collected back to the driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2550b511-f67e-4573-8824-fd3e74dbb43f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "partitioned_sequence_pairs_dlcs.write.parquet(results_dataset, mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd1ff3d8-f1dc-43a7-a7e6-3a123124a82e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Appendix: Using rdd.map()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83815a5f-ed14-4000-9bdb-af147ebfe5e2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "the `rdd.map()` style takes essentially the same time, as under the hood it is actually what is executed on the dataframe\n",
    "\n",
    "however in this case we need to manually \"carry over\" the key (patient_id) as the `map()` operation only returns the actual value of the `dlcs()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c819bd7-5176-490e-a18e-332f27c9cf7a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "partitioned_sequence_pairs_dlcs  = partitioned_sequence_pairs.rdd.map(lambda row: dlcs(row['read_2'], row['read_2_1'])).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84fb0da4-197e-4dbc-9925-4bd7f815ecd1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "managing RDDs is less intuitive and effectively the Dataframes / SQL API is much more friendly. \n",
    "\n",
    "Thus we do not repeat the analytics above here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "137b499a-bf54-4e7d-85a7-f3d3d7069778",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**WARNING** the `collect()` action forces a complete computation and also transfers all data back to driver. \n",
    "\n",
    "This is the most expensive operation, and should be avoided if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8898f1e1-88ec-4864-a73f-82ad6d351d64",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "partitioned_sequence_pairs_dlcs.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b69c336-7aad-4bba-978b-1f750f531dae",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "writing the result back to file also forces a complete computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d43f0f75-490d-4888-851b-28021075e76c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "parquet_file_dlcs = '/FileStore/tables/patients-sequences/patients_df_sampled_1000_dlcs.parquet'\n",
    "partitioned_sequence_pairs_dlcs.write.parquet(parquet_file_dlcs, mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12d95312-cde8-4b56-9af6-1477e726bdd3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Appendix: using Databricks native Delta Tables\n",
    "\n",
    "here instead of writing the crossjoin file to parquet (partitioned), we write it to Delta Tables. \n",
    "\n",
    "The goal is to check whether this results in faster reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e731ee83-58f7-4927-848f-e8ea3b5c4ec8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "partitioned_sequences_table = '/FileStore/tables/patients-sequences/patients_df_sampled_10000.delta'\n",
    "\n",
    "partitioned_sequence_pairs.write.format(\"delta\").mode(\"overwrite\").save(sequences_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "710e586b-cbb2-46f1-acd3-6ad0e4de0f11",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sequence_pairs_delta = spark.read.format(\"delta\").load(partitioned_sequences_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4cade15-23fc-46b8-acce-aa564f90df1a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sequence_pairs_delta.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d1a126c-9ff0-47a3-872a-39c32e327545",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "partitioned_sequence_pairs_dlcs = partitioned_sequence_pairs.withColumn('dlcs', dlcs_udf('read_2','read_2_1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3946519c-bf43-43a7-8f36-b7be32ece0af",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "partitioned_sequence_pairs_dlcs.show(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b4e56f0-68bf-4d41-8cdd-23e924be1c6d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dlcs_delta_file = '/FileStore/tables/patients-sequences/patients_df_sampled_dlcs_partitioned_10000.delta'\n",
    "partitioned_sequence_pairs_dlcs.write.format(\"delta\").mode(\"overwrite\").save(dlcs_delta_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "061e708d-da39-42e4-9bba-f967e19bf23a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [
    {
     "elements": [
      {
       "dashboardResultIndex": 0,
       "elementNUID": "bf6490cd-0fab-4fbb-b62d-eff4567afa8e",
       "elementType": "command",
       "guid": "12d64963-222a-4f79-932a-3d167b586890",
       "options": null,
       "position": {
        "height": 6,
        "width": 12,
        "x": 0,
        "y": 0,
        "z": null
       },
       "resultIndex": null
      }
     ],
     "globalVars": {},
     "guid": "",
     "layoutOption": {
      "grid": true,
      "stack": true
     },
     "nuid": "e10f06bc-ea0b-4f1f-a2b1-370b4e56bd23",
     "origId": 2916125345561136,
     "title": "Untitled",
     "version": "DashboardViewV1",
     "width": 1024
    }
   ],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "dLCS-Spark",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
